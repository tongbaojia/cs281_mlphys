{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer, scale\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import LogNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sys import argv\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb1aea4de10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(24)\n",
    "torch.manual_seed(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline\n",
    "bkg_WZlvll.csv\n",
    "(6713.955682059312, 387.68207095283077)\n",
    "Original: 6713.956 Final 387.682 Eff 5.774\n",
    "SFOS0 Original: 58.512 Final 30.040 Eff 51.340\n",
    "SFOS1 Original: 3270.974 Final 171.423 Eff 5.241\n",
    "SFOS2 Original: 3355.089 Final 163.936 Eff 4.886\n",
    "bkg_WZqqll.csv\n",
    "(0.008658278097300479, 0.00028139514870417006)\n",
    "Original: 0.009 Final 0.000 Eff 3.250\n",
    "SFOS0 Original: 0.000 Final 0.000 Eff 0.000\n",
    "SFOS1 Original: 0.004 Final 0.000 Eff 1.875\n",
    "SFOS2 Original: 0.004 Final 0.000 Eff 4.036\n",
    "signal_WpWpWm.csv\n",
    "(24.15797396656229, 10.584578019860624)\n",
    "Original: 24.158 Final 10.585 Eff 43.814\n",
    "SFOS0 Original: 5.949 Final 3.486 Eff 58.602\n",
    "SFOS1 Original: 11.973 Final 4.798 Eff 40.075\n",
    "SFOS2 Original: 6.194 Final 2.271 Eff 36.659\n",
    "signal_WmWpWm.csv\n",
    "(14.276670433050755, 6.16103801415841)\n",
    "Original: 14.277 Final 6.161 Eff 43.155\n",
    "SFOS0 Original: 3.484 Final 2.020 Eff 57.993\n",
    "SFOS1 Original: 7.054 Final 2.779 Eff 39.405\n",
    "SFOS2 Original: 3.712 Final 1.341 Eff 36.140\n",
    "Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only WZ background\n",
    "baseline_signal_eff = (10.585 + 6.161) / (24.158 + 14.277)\n",
    "baseline_bg_eff = 387.682/6713.956\n",
    "baseline_signif = [0.46, 0.4148, 0.15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more background\n",
    "baseline_signal_eff = 0.42113\n",
    "baseline_bg_eff = 0.03809\n",
    "baseline_signif = [1.44, 0.57, 0.28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m(x, w):\n",
    "    \"\"\"Weighted Mean\"\"\"\n",
    "    return np.sum(x * w) / np.sum(w)\n",
    "\n",
    "def cov(x, y, w):\n",
    "    \"\"\"Weighted Covariance\"\"\"\n",
    "    return np.sum(w * (x - m(x, w)) * (y - m(y, w))) / np.sum(w)\n",
    "\n",
    "def corr(x, y, w):\n",
    "    \"\"\"Weighted Correlation\"\"\"\n",
    "    return cov(x, y, w) / np.sqrt(cov(x, x, w) * cov(y, y, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_dir):\n",
    "    fs = [data_dir + f for f in os.listdir(data_dir) if ('signal' in f or 'WZ' in f) and f[0] != '.']\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for f in fs:\n",
    "        print f\n",
    "        new_df = pd.read_csv(f)\n",
    "        df = pd.concat([df, new_df], ignore_index = True)\n",
    "        df.index = range(len(df))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_more_bg(data_dir):\n",
    "    fs = [data_dir + f for f in os.listdir(data_dir) if f[0] != '.']\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for f in fs:\n",
    "        print f\n",
    "        new_df = pd.read_csv(f)\n",
    "        df = pd.concat([df, new_df], ignore_index = True)\n",
    "        df.index = range(len(df))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cl_ix(df):\n",
    "    df['is_sig'] = [1 if 'signal' in val else 0 for val in df.cl.values]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WWdataset(Dataset):\n",
    "\n",
    "    def __init__(self, pd_dataset, non_input=None):\n",
    "        self.dataset = pd_dataset\n",
    "        \n",
    "        self.non_input = ['runNumber', 'lbNumber', 'eventNumber', 'SFOS', 'is_sig', 'weight', 'cl', 'preds']\n",
    "        if self.non_input is not None:\n",
    "            self.non_input += non_input\n",
    "        self.input_vars = [col for col in self.dataset.columns if not col in self.non_input]\n",
    "        \n",
    "        self.scale_dataset()\n",
    "        \n",
    "        self.target_var = 'is_sig'\n",
    "        self.weight_var = 'weight'\n",
    "        \n",
    "        self.input_np = self.dataset[self.input_vars].as_matrix().astype(dtype=np.float32)\n",
    "        self.target_np = self.dataset[self.target_var].as_matrix().astype(dtype=int)\n",
    "        self.weight_np = self.dataset[self.weight_var].as_matrix().astype(dtype=np.float32)\n",
    "\n",
    "        self.inputs = torch.from_numpy(self.input_np)\n",
    "        self.target = torch.from_numpy(self.target_np)\n",
    "        self.weight = torch.from_numpy(self.weight_np)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.inputs[idx]\n",
    "        target = self.target[idx]\n",
    "        weight = self.weight[idx]\n",
    "        return inputs, target, weight\n",
    "\n",
    "    def n_input(self):\n",
    "        return len(self.input_vars)\n",
    "    \n",
    "    def scale_dataset(self):\n",
    "        continuous_cols = [column for column in self.input_vars\n",
    "                           if self.dataset[column].dtype is np.dtype('float64')]\n",
    "        self.dataset.loc[:, continuous_cols] = scale(self.dataset[continuous_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_logistic_regression(n_input):\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(n_input, 2),\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_deep_logistic_regression(n_input):\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(n_input, 200),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(200, 100),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(100, 10),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(10, 2),\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgb_classifier(train_dataset):\n",
    "    model = XGBClassifier()\n",
    "    model.fit(train_dataset.input_np, train_dataset.target_np, sample_weight=train_dataset.weight_np)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(pandas_dataset, non_input=None, training_fraction=0.7):\n",
    "    n_switch_train = int( len(pandas_dataset) * training_fraction)\n",
    "    n_switch_test = int( len(pandas_dataset) * (training_fraction+1.)/2.)\n",
    "\n",
    "    train_dataset_aux = pandas_dataset[:n_switch_train]\n",
    "    test_dataset_aux = pandas_dataset[n_switch_train:n_switch_test]\n",
    "    val_dataset_aux = pandas_dataset[n_switch_test:]\n",
    "\n",
    "    train_dataset = WWdataset(train_dataset_aux, non_input)\n",
    "    test_dataset = WWdataset(test_dataset_aux, non_input)\n",
    "    val_dataset = WWdataset(val_dataset_aux, non_input)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=200, shuffle=True, num_workers=2)\n",
    "    testloader = torch.utils.data.DataLoader(test_dataset, batch_size=200, shuffle=True, num_workers=2)    \n",
    "    valloader = torch.utils.data.DataLoader(val_dataset, batch_size=200, shuffle=True, num_workers=2)\n",
    "    \n",
    "    return train_dataset, test_dataset, val_dataset, trainloader, testloader, valloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/bkg_WZqqll.csv\n",
      "../../data/signal_WmWpWm.csv\n",
      "../../data/bkg_WZlvll.csv\n",
      "../../data/signal_WpWpWm.csv\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../../data/\"\n",
    "pandas_dataset = add_cl_ix(get_data(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python2.7/site-packages/pandas/core/indexing.py:537: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "non_input = ['l0_m', 'l1_m', 'l2_m', 'SF0S']\n",
    "train_fraction = 0.7\n",
    "pandas_dataset = shuffle(pandas_dataset)\n",
    "train_dataset, test_dataset, val_dataset, trainloader, testloader, valloader = split_dataset(pandas_dataset, non_input=non_input, training_fraction=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_logi = net_logistic_regression(train_dataset.n_input())\n",
    "net_deep = net_deep_logistic_regression(train_dataset.n_input())\n",
    "criterion = nn.CrossEntropyLoss(reduce=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_batch(batch, net):\n",
    "    inputs, label, weight = batch\n",
    "    inputs, label, weight = Variable(inputs), Variable(label), Variable(weight)\n",
    "\n",
    "    output = net(inputs)\n",
    "    losses = criterion(output, label.squeeze())\n",
    "    loss = (losses * weight.squeeze().float()).sum()\n",
    "\n",
    "    return loss\n",
    "\n",
    "def compute_loss_epoch(loader, net):\n",
    "    loss = 0.\n",
    "    for i, batch in enumerate(loader):\n",
    "        loss += compute_loss_batch(batch, net).data[0]\n",
    "    loss /= len(loader)\n",
    "    return loss\n",
    "\n",
    "def train_model(trainloader, valloader, net, criterion, n_epochs=6):\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    for epoch in range(n_epochs):\n",
    "        print\n",
    "        print \"epoch: \", epoch\n",
    "        running_loss = 0.\n",
    "        for i, batch in enumerate(trainloader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = compute_loss_batch(batch, net)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.data[0]\n",
    "            # if i % 200 == 199:    # print every 200 mini-batches\n",
    "            #   print \"batch:  {}, loss: {}\".format(i+1, running_loss/(i+1))\n",
    "        print \"running loss: \", running_loss/len(trainloader)\n",
    "        print \"train loss: \", compute_loss_epoch(trainloader, net)        \n",
    "        print \"val loss: \", compute_loss_epoch(valloader, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:  0\n",
      "running loss:  0.402998465352\n",
      "train loss:  0.16977388788\n",
      "val loss:  0.169062081594\n",
      "\n",
      "epoch:  1\n",
      "running loss:  0.129666190499\n",
      "train loss:  0.103938069092\n",
      "val loss:  0.103918373949\n",
      "\n",
      "epoch:  2\n",
      "running loss:  0.0925051869499\n",
      "train loss:  0.0850402505966\n",
      "val loss:  0.0851989223737\n",
      "\n",
      "epoch:  3\n",
      "running loss:  0.0823529736865\n",
      "train loss:  0.0809256117325\n",
      "val loss:  0.0811689295535\n",
      "\n",
      "epoch:  4\n",
      "running loss:  0.0805024731811\n",
      "train loss:  0.0802936864495\n",
      "val loss:  0.0804698730109\n",
      "\n",
      "epoch:  5\n",
      "running loss:  0.0803891572976\n",
      "train loss:  0.0803960629245\n",
      "val loss:  0.0806456415465\n",
      "\n",
      "epoch:  6\n",
      "running loss:  0.0803545718654\n",
      "train loss:  0.0803282670393\n",
      "val loss:  0.0805840843242\n",
      "\n",
      "epoch:  7\n",
      "running loss:  0.0803647821695\n",
      "train loss:  0.0803223709636\n",
      "val loss:  0.0804614576756\n"
     ]
    }
   ],
   "source": [
    "train_model(trainloader, valloader, net_logi, criterion, n_epochs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train deep logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:  0\n",
      "running loss:  0.085513837581\n",
      "train loss:  0.0720925331991\n",
      "val loss:  0.0722656148922\n",
      "\n",
      "epoch:  1\n",
      "running loss:  0.0681155179441\n",
      "train loss:  0.064481918857\n",
      "val loss:  0.0649933545505\n",
      "\n",
      "epoch:  2\n",
      "running loss:  0.0623335137879\n",
      "train loss:  0.0595107162134\n",
      "val loss:  0.0606943776479\n",
      "\n",
      "epoch:  3\n",
      "running loss:  0.0591483161487\n",
      "train loss:  0.0585852755271\n",
      "val loss:  0.0598406358372\n",
      "\n",
      "epoch:  4\n",
      "running loss:  0.0575231956358\n",
      "train loss:  0.0565986203514\n",
      "val loss:  0.0580559552697\n",
      "\n",
      "epoch:  5\n",
      "running loss:  0.0564600124699\n",
      "train loss:  0.0559511976046\n",
      "val loss:  0.0574116461578\n",
      "\n",
      "epoch:  6\n",
      "running loss:  0.0559374474469\n",
      "train loss:  0.0559973945776\n",
      "val loss:  0.0578316785135\n",
      "\n",
      "epoch:  7\n",
      "running loss:  0.055420013627\n",
      "train loss:  0.0537893363456\n",
      "val loss:  0.0559175312823\n"
     ]
    }
   ],
   "source": [
    "train_model(trainloader, valloader, net_deep, criterion, n_epochs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = train_xgb_classifier(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction and Signficance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prediction(dataset, net, field_name):\n",
    "    input_for_pred = Variable(dataset.inputs)\n",
    "    predicted_scores = net(input_for_pred)\n",
    "    predicted_prob = nn.functional.softmax(predicted_scores, dim=1)\n",
    "    dataset.dataset.loc[:, field_name] = predicted_prob.data.numpy()[:,1]\n",
    "\n",
    "def add_xgb_prediction(dataset, model, field_name):\n",
    "    out = model.predict_proba(dataset.input_np)[:, 1]\n",
    "    dataset.dataset.loc[:, field_name] = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python2.7/site-packages/pandas/core/indexing.py:357: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n"
     ]
    }
   ],
   "source": [
    "add_prediction(train_dataset, net_logi, 'logi_pred')\n",
    "add_prediction(val_dataset, net_logi, 'logi_pred')\n",
    "add_prediction(test_dataset, net_logi, 'logi_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_prediction(train_dataset, net_deep, 'deep_pred')\n",
    "add_prediction(val_dataset, net_deep, 'deep_pred')\n",
    "add_prediction(test_dataset, net_deep, 'deep_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_xgb_prediction(train_dataset, xgb_model, 'xgb_pred')\n",
    "add_xgb_prediction(val_dataset, xgb_model, 'xgb_pred')\n",
    "add_xgb_prediction(test_dataset, xgb_model, 'xgb_pred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt4Agg\n"
     ]
    }
   ],
   "source": [
    "mpl.rcParams['font.size'] = 14\n",
    "%matplotlib auto\n",
    "\n",
    "df = val_dataset.dataset\n",
    "\n",
    "preds = [\n",
    "    'logi_pred', \n",
    "    'deep_pred', \n",
    "    'xgb_pred'\n",
    "]\n",
    "legend_titles = {\n",
    "    'logi_pred': '1-layer NN', \n",
    "    'deep_pred': '4-layers NN', \n",
    "    'xgb_pred': 'BDT', \n",
    "}\n",
    "colors = ['r', 'b', 'g']\n",
    "rocs = {}\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.title('ROC comparison, validation sample')\n",
    "plt.xlim((0,1))\n",
    "plt.ylim((0,1))\n",
    "plt.plot([0,1], [0,1], linestyle='--')\n",
    "plt.plot([baseline_bg_eff], [baseline_signal_eff], marker='o', markersize=3, color='k', label='baseline')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid()\n",
    "\n",
    "for i, var_pred in enumerate(preds):\n",
    "    rocs[var_pred] = metrics.roc_curve(df['is_sig'].values, df[var_pred].values, sample_weight=df['weight'].values, pos_label=1)\n",
    "    fpr, tpr, thresholds = rocs[var_pred]\n",
    "    area = metrics.roc_auc_score(df['is_sig'].values, df[var_pred].values, sample_weight=df['weight'].values)\n",
    "    # area = metrics.auc(fpr, tpr, reorder=True)\n",
    "    plt.plot(fpr, tpr, label=legend_titles[var_pred]+', area: '+str(round(area, 2)), color=colors[i])\n",
    "    \n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "#plt.savefig(\"./plots/roc_curve_train_nn_regression.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute signal, background and significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_sig_n_bkg(df, SF0S, n_bins, plot):\n",
    "    df = df[df['SFOS'] == SF0S] \n",
    "\n",
    "    x_bins = np.linspace(0, 1, n_bins)\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    gs = gridspec.GridSpec(3, 1)\n",
    "    ax = plt.subplot(gs[0,:])\n",
    "    plt.title('NN Regression Separability on train sample, SF0S = {}'.format(SF0S))\n",
    "    # n_bkg,bins,paint = plt.hist(df[df.is_sig == 0].preds, bins=x_bins, weights=df[df.is_sig == 0].weight, color='r')\n",
    "    n_bkg,bins,paint = plt.hist(df[df.is_sig == 0].preds.as_matrix(), bins=x_bins, weights=df[df.is_sig == 0].weight.as_matrix(), color='r')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.yscale('log')\n",
    "    plt.ylabel(r'Weighted Background Counts', size=9)\n",
    "    plt.legend(handles=[mpatches.Patch(color='red', label='Background')])\n",
    "    ax1 = plt.subplot(gs[1,:])\n",
    "    n_sig,bins,paint = plt.hist(df[df.is_sig == 1].preds.as_matrix(), bins=x_bins, weights=df[df.is_sig == 1].weight.as_matrix(), color='g')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.yscale('log')\n",
    "    plt.ylabel(r'Weighted Signal Counts', size=9)\n",
    "    plt.legend(handles=[mpatches.Patch(color='green', label='Signal')])\n",
    "    ax2 = plt.subplot(gs[2,:])\n",
    "    plt.bar((x_bins[:-1] + x_bins[1:]) / 2., n_sig / np.sqrt(n_bkg), width=x_bins[1] - x_bins[0], color='k')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylabel(r'Significance ($S/\\sqrt{B})$', size=9)\n",
    "    plt.xlabel('Probability Event is a Signal')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if plot==True:\n",
    "        plt.show()\n",
    "    return n_sig, n_bkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_significance(sig, bg):\n",
    "    return np.sqrt(2*( (sig + bg)*np.log(1+sig/bg) -sig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dataset = val_dataset.dataset\n",
    "#pandas_dataset = pd.concat([test_dataset.dataset, train_dataset.dataset])\n",
    "#pandas_dataset = pd.concat([test_dataset.dataset, train_dataset.dataset, val_dataset.dataset])\n",
    "scale_factor = 1./0.15\n",
    "pred_variable = 'deep_pred'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt4Agg\n"
     ]
    }
   ],
   "source": [
    "mpl.rcParams['font.size'] = 14\n",
    "%matplotlib auto\n",
    "significance = []\n",
    "signal = []\n",
    "background = []\n",
    "probability_cuts = np.linspace(0, 1, 20)\n",
    "for SF0S in range(3):\n",
    "    significance.append([])\n",
    "    signal.append([])\n",
    "    background.append([])\n",
    "    for p in probability_cuts:\n",
    "        sig = pandas_dataset[(pandas_dataset['SFOS'] == SF0S)\n",
    "                                 & (pandas_dataset[pred_variable] > p)\n",
    "                                 & (pandas_dataset['is_sig'] == 1)]['weight'].sum()\n",
    "        bg = pandas_dataset[(pandas_dataset['SFOS'] == SF0S)\n",
    "                                 & (pandas_dataset[pred_variable] > p)\n",
    "                                 & (pandas_dataset['is_sig'] == 0)]['weight'].sum()\n",
    "        \n",
    "        # significance[SF0S].append(sig/np.sqrt(bg))\n",
    "        significance[SF0S].append( compute_significance(scale_factor*sig, scale_factor*bg) )\n",
    "        \n",
    "        signal[SF0S].append(sig)\n",
    "        background[SF0S].append(bg)\n",
    "    plt.plot(probability_cuts, significance[SF0S], label=\"SF0S = {}\".format(SF0S))\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.title('Significance varying cuts and for various SF0S')\n",
    "plt.xlabel('Classifier output')\n",
    "plt.ylabel('Signficance')\n",
    "plt.xlim((0,1))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt4Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib auto\n",
    "mpl.rcParams['font.size'] = 12\n",
    "significance = []\n",
    "signal = []\n",
    "background = []\n",
    "probability_cuts = np.linspace(0, 1, 50)\n",
    "colors = ['b', 'r', 'g']\n",
    "total_bs_significance = 0.\n",
    "\n",
    "for SF0S in range(3):\n",
    "    significance.append([])\n",
    "    signal.append([])\n",
    "    background.append([])\n",
    "    for p in probability_cuts:\n",
    "        sig = pandas_dataset[(pandas_dataset['SFOS'] == SF0S)\n",
    "                                 & (pandas_dataset[pred_variable] > p)\n",
    "                                 & (pandas_dataset['is_sig'] == 1)]['weight'].sum()\n",
    "        bg = pandas_dataset[(pandas_dataset['SFOS'] == SF0S)\n",
    "                                 & (pandas_dataset[pred_variable] > p)\n",
    "                                 & (pandas_dataset['is_sig'] == 0)]['weight'].sum()\n",
    "        \n",
    "        # significance[SF0S].append(sig/np.sqrt(bg))\n",
    "        significance[SF0S].append(compute_significance(scale_factor*sig, scale_factor*bg))\n",
    "        \n",
    "        signal[SF0S].append(sig)\n",
    "        background[SF0S].append(bg)\n",
    "    plt.plot(probability_cuts, significance[SF0S], label=\"SF0S = {}\".format(SF0S), color=colors[SF0S])\n",
    "    plt.plot([0,1], [baseline_signif[SF0S], baseline_signif[SF0S]], \n",
    "             linestyle='--', \n",
    "             label=\"baseline SF0S = {}\".format(SF0S), \n",
    "             color=colors[SF0S])\n",
    "\n",
    "# plt.plot([0,1], [total_bs_significance, total_bs_significance], \n",
    "#             linestyle='--', \n",
    "#             label=\"total baseline\", \n",
    "#             color='black')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.title('Significance as a function of the classifer cut', fontsize=16)\n",
    "plt.xlabel('Classifier output', fontsize=16)\n",
    "plt.ylabel('Signficance', fontsize=16)\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((0, 2.2))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for SF0S in range(3):\n",
    "    plt.plot(probability_cuts, background[SF0S], label=\"SF0S = {}\".format(SF0S))\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.title('Number of background events varying cuts and for various SF0S')\n",
    "plt.xlabel('Classifier output')\n",
    "plt.ylabel('Number of background events')\n",
    "plt.xlim((0,1))\n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for SF0S in range(3):\n",
    "    plt.plot(probability_cuts, signal[SF0S], label=\"SF0S = {}\".format(SF0S))\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.title('Number of signal events varying cuts and for various SF0S')\n",
    "plt.xlabel('Classifier output')\n",
    "plt.ylabel('Number of signal events')\n",
    "plt.xlim((0,1))\n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2Dplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dataset = val_dataset.dataset\n",
    "pred_variable = 'deep_pred'\n",
    "\n",
    "figures = []\n",
    "for var in val_dataset.input_vars:\n",
    "    colors = ['r', 'b']\n",
    "    title = {0: 'background', 1: 'signal'}\n",
    "    \n",
    "    f, axes = plt.subplots(1, 2, sharey=True)\n",
    "    for sig in [0, 1]:        \n",
    "        x = pandas_dataset[pandas_dataset['is_sig'] == sig][var]\n",
    "        y = pandas_dataset[pandas_dataset['is_sig'] == sig][pred_variable]\n",
    "        weight = pandas_dataset[pandas_dataset['is_sig'] == sig]['weight']\n",
    "        \n",
    "        h = axes[sig].hist2d(x, y, weights=weight, bins=20, norm=LogNorm())\n",
    "        axes[sig].set_ylabel('classifier output')\n",
    "        axes[sig].set_xlabel(var)\n",
    "        axes[sig].set_xlim([-3, 20])\n",
    "        axes[sig].set_title(title[sig] + ', corr: {:.2f}'.format(corr(x, y, weight)))\n",
    "    #f.colorbar(h[3]) \n",
    "    # plt.show()\n",
    "    figures.append(f)\n",
    "\n",
    "import matplotlib.backends.backend_pdf\n",
    "pdf = matplotlib.backends.backend_pdf.PdfPages(\"2D_plots.pdf\")\n",
    "for fig in figures:\n",
    "    pdf.savefig( fig )\n",
    "pdf.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
